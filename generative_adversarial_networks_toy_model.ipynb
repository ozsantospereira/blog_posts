{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70777a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7894962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T20:15:53.779737Z",
     "start_time": "2025-02-22T20:15:50.340738Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from scipy.stats import norm\n",
    "%matplotlib notebook\n",
    "\n",
    "# ---------------------------\n",
    "# Data and Simulation Functions\n",
    "# ---------------------------\n",
    "def compute_ground_truth(x, mean=5, std=1):\n",
    "    \"\"\"\n",
    "    Compute the ground truth PDF for x using a normal distribution.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): 1D array of x-values.\n",
    "        mean (float): Mean of the real distribution.\n",
    "        std (float): Standard deviation of the real distribution.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Ground truth PDF values.\n",
    "    \"\"\"\n",
    "    return norm.pdf(x, mean, std)\n",
    "\n",
    "def compute_generator_pdf(x, frame, total_frames, init_mean=2, init_std=2, target_mean=5, target_std=1):\n",
    "    \"\"\"\n",
    "    Compute the generator's PDF for the given frame by linearly interpolating \n",
    "    its parameters from initial to target values. This makes the generated data \n",
    "    converge to the ground truth.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): 1D array of x-values.\n",
    "        frame (int): Current frame (epoch) index.\n",
    "        total_frames (int): Total number of frames.\n",
    "        init_mean (float): Initial mean of the generator's distribution.\n",
    "        init_std (float): Initial standard deviation of the generator's distribution.\n",
    "        target_mean (float): Target mean (should match ground truth).\n",
    "        target_std (float): Target standard deviation (should match ground truth).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Generator PDF values for this frame.\n",
    "    \"\"\"\n",
    "    t = frame / (total_frames - 1)  # t varies from 0 to 1\n",
    "    gen_mean = init_mean + (target_mean - init_mean) * t\n",
    "    gen_std = init_std + (target_std - init_std) * t\n",
    "    return norm.pdf(x, gen_mean, gen_std)\n",
    "\n",
    "def compute_discriminator_prob(real_pdf, gen_pdf, factor=10):\n",
    "    \"\"\"\n",
    "    Simulate the discriminator's output as a logistic function of the difference\n",
    "    between generator and real PDFs.\n",
    "    \n",
    "    Args:\n",
    "        real_pdf (np.ndarray): Ground truth PDF values.\n",
    "        gen_pdf (np.ndarray): Generator PDF values.\n",
    "        factor (float): Scaling factor for the logistic function.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Discriminator probability outputs.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-(gen_pdf - real_pdf) * factor))\n",
    "\n",
    "def simulate_training(x, num_frames):\n",
    "    \"\"\"\n",
    "    Simulate the training process over a specified number of frames.\n",
    "    The generated PDF converges linearly to the ground truth PDF.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): 1D array of x-values.\n",
    "        num_frames (int): Number of simulation frames.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ground_truth_data, generator_data, discriminator_data)\n",
    "               Each is a NumPy array of shape (num_frames, len(x)).\n",
    "    \"\"\"\n",
    "    ground_truth_list = []\n",
    "    generator_list = []\n",
    "    discriminator_list = []\n",
    "    \n",
    "    # Ground truth PDF remains constant throughout the simulation\n",
    "    real_pdf = compute_ground_truth(x)\n",
    "    \n",
    "    for frame in range(num_frames):\n",
    "        gen_pdf = compute_generator_pdf(x, frame, num_frames)\n",
    "        disc_pdf = compute_discriminator_prob(real_pdf, gen_pdf)\n",
    "        \n",
    "        ground_truth_list.append(real_pdf)\n",
    "        generator_list.append(gen_pdf)\n",
    "        discriminator_list.append(disc_pdf)\n",
    "    \n",
    "    return (np.array(ground_truth_list), \n",
    "            np.array(generator_list), \n",
    "            np.array(discriminator_list))\n",
    "\n",
    "# ---------------------------\n",
    "# Animation Functions\n",
    "# ---------------------------\n",
    "def create_animation(x, ground_truth_data, generator_data, discriminator_data, num_frames, interval=200):\n",
    "    \"\"\"\n",
    "    Create an animation using precomputed data.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): 1D array of x-values.\n",
    "        ground_truth_data (np.ndarray): Array of ground truth PDFs (num_frames x len(x)).\n",
    "        generator_data (np.ndarray): Array of generator PDFs (num_frames x len(x)).\n",
    "        discriminator_data (np.ndarray): Array of discriminator outputs (num_frames x len(x)).\n",
    "        num_frames (int): Number of frames.\n",
    "        interval (int): Delay between frames in milliseconds.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ani, fig) where ani is the FuncAnimation object and fig is the figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(np.min(x), np.max(x))\n",
    "    ax.set_ylim(0, 0.8)\n",
    "    ax.set_title(\"Training Discriminator D\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    \n",
    "    real_line, = ax.plot([], [], 'k--', label='Real Data (pdata)')\n",
    "    fake_line, = ax.plot([], [], 'g-', label='Generator (pg)')\n",
    "    disc_line, = ax.plot([], [], 'b--', label='Discriminator (D)')\n",
    "    ax.legend()\n",
    "    \n",
    "    def update(frame):\n",
    "        real_line.set_data(x, ground_truth_data[frame])\n",
    "        fake_line.set_data(x, generator_data[frame])\n",
    "        disc_line.set_data(x, discriminator_data[frame])\n",
    "        return real_line, fake_line, disc_line\n",
    "    \n",
    "    ani = animation.FuncAnimation(fig, update, frames=num_frames, interval=interval, blit=True)\n",
    "    return ani, fig\n",
    "\n",
    "def save_animation(ani, mp4_filename, gif_filename, fps=10):\n",
    "    \"\"\"\n",
    "    Save the animation in both MP4 and GIF formats.\n",
    "    \n",
    "    Args:\n",
    "        ani (FuncAnimation): The animation object.\n",
    "        mp4_filename (str): Output filename for MP4.\n",
    "        gif_filename (str): Output filename for GIF.\n",
    "        fps (int): Frames per second.\n",
    "    \"\"\"\n",
    "    writer = FFMpegWriter(fps=fps, metadata=dict(artist='Your Name'),\n",
    "                           extra_args=['-vcodec', 'libx264'])\n",
    "    ani.save(mp4_filename, writer=writer)\n",
    "    ani.save(gif_filename, writer='pillow', fps=fps)\n",
    "\n",
    "# ---------------------------\n",
    "# Direct Function Calls (No main function)\n",
    "# ---------------------------\n",
    "\n",
    "# Define x-range and simulation parameters\n",
    "x = np.linspace(-2, 12, 100)\n",
    "num_frames = 30\n",
    "\n",
    "# Simulate training: all data is stored in arrays\n",
    "ground_truth_data, generator_data, discriminator_data = simulate_training(x, num_frames)\n",
    "print(\"Ground truth data shape:\", ground_truth_data.shape)\n",
    "print(\"Generator data shape:\", generator_data.shape)\n",
    "print(\"Discriminator data shape:\", discriminator_data.shape)\n",
    "\n",
    "# Create and display the animation\n",
    "ani, fig = create_animation(x, ground_truth_data, generator_data, discriminator_data, num_frames)\n",
    "plt.show()\n",
    "\n",
    "# Save the animation in MP4 and GIF formats\n",
    "save_animation(ani, \"gan_training_progress.mp4\", \"gan_training_progress.gif\", fps=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c012e",
   "metadata": {},
   "source": [
    "# Explanation of the Code and Functional Analysis\n",
    "\n",
    "The provided code implements a simplified Generative Adversarial Network (GAN) training setup, where two generator models are trained independently to approximate a target distribution. The target distribution is a univariate normal distribution with mean $\\mu = 5$ and standard deviation $\\sigma = 1$. The training process involves generating synthetic samples and optimizing the generators using a simple loss function. Additionally, an animated visualization is created to illustrate the evolution of the generator's output over time. The function `true_distribution(n)` generates $n$ samples from the real distribution, defined as:\n",
    "\n",
    "$$\n",
    "X \\sim \\mathcal{N}(\\mu=5, \\sigma=1)\n",
    "$$\n",
    "\n",
    "where samples are drawn from a normal distribution and reshaped into a column vector. The function `build_generator()` defines a neural network serving as the generator. This model is a feedforward neural network composed of three fully connected (`Linear`) layers with ReLU activation in the hidden layers. Mathematically, the generator transforms a latent vector $z$ sampled from a standard normal distribution:\n",
    "\n",
    "$$\n",
    "z \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "into an output approximating the real data distribution. The forward pass of the generator is given by:\n",
    "\n",
    "$$\n",
    "h_1 = \\text{ReLU}(W_1 z + b_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\text{ReLU}(W_2 h_1 + b_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = W_3 h_2 + b_3\n",
    "$$\n",
    "\n",
    "where $W_i$ and $b_i$ are the learnable weight matrices and bias vectors, respectively. The function `train_generator(generator, optimizer)` implements a single training step for the generator. The key steps are:\n",
    "\n",
    "1. A batch of random latent vectors is sampled:\n",
    "\n",
    "   $$\n",
    "   Z \\sim \\mathcal{N}(0, I)\n",
    "   $$\n",
    "\n",
    "2. The generator maps these latent vectors to synthetic data points:\n",
    "\n",
    "   $$\n",
    "   \\hat{X} = G(Z)\n",
    "   $$\n",
    "\n",
    "3. The loss function is defined as the negative mean of the generated samples:\n",
    "\n",
    "   $$\n",
    "   \\mathcal{L}_G = -\\mathbb{E}[\\hat{X}]\n",
    "   $$\n",
    "\n",
    "   This loss function incentivizes the generator to shift its outputs towards higher values to match the real distribution (which has a mean of 5). The gradients are computed, and the optimizer updates the model parameters using backpropagation.\n",
    "\n",
    "Two generators (`generator1` and `generator2`) are instantiated, each with its own optimizer (`Adam`). The training loop runs for `epochs = 200`, during which both generators produce synthetic data, and their parameters are updated iteratively. The function `update(frame, x, real_pdf, fake_line, disc_line)` simulates the evolution of the generator and discriminator:\n",
    "\n",
    "1. The generator's output distribution is modeled by a normal distribution:\n",
    "\n",
    "   $$\n",
    "   G(x) \\sim \\mathcal{N}(\\mu_G, \\sigma_G)\n",
    "   $$\n",
    "\n",
    "   where $\\mu_G$ increases over time, and $\\sigma_G$ decreases, simulating convergence toward the true distribution.\n",
    "\n",
    "2. The discriminator's response is approximated using a logistic function:\n",
    "\n",
    "   $$\n",
    "   D(x) = \\frac{1}{1 + e^{-10(G(x) - p_{\\text{data}}(x))}}\n",
    "   $$\n",
    "\n",
    "   which models the probability that a given sample is real.\n",
    "\n",
    "The `FuncAnimation` module creates an animated visualization, updating the generator and discriminator distributions over time. The animation is saved in both MP4 and GIF formats. This code implements a fundamental GAN training procedure with two generators independently learning to approximate a real data distribution. The model uses a simple mean-based loss function instead of adversarial training with a discriminator. The visualization provides an intuitive representation of how the generator improves over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe4cc31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T22:18:32.712590Z",
     "start_time": "2025-02-22T22:18:30.342593Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from scipy.stats import norm\n",
    "%matplotlib notebook\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 10       # Dimensionality of the latent (noise) vector\n",
    "hidden_dim = 128      # Number of neurons in the hidden layers\n",
    "data_dim = 1          # Output dimensionality (univariate distribution)\n",
    "batch_size = 128      # Number of samples per training batch\n",
    "epochs = 200          # Number of training epochs\n",
    "lr = 0.0002           # Learning rate for the optimizers\n",
    "\n",
    "def true_distribution(n):\n",
    "    \"\"\"\n",
    "    Generate samples from the true (real) distribution.\n",
    "    \n",
    "    Args:\n",
    "        n (int): Number of samples to generate.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (n, 1) with samples drawn from a normal \n",
    "                    distribution (mean=5, std=1).\n",
    "    \"\"\"\n",
    "    mean = 5\n",
    "    std = 1\n",
    "    return np.random.normal(mean, std, n).reshape(-1, 1)\n",
    "\n",
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Build the generator model using a functional approach.\n",
    "    \n",
    "    Returns:\n",
    "        torch.nn.Sequential: The generator model mapping latent vectors to data.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(latent_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, data_dim),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_generator(generator, optimizer):\n",
    "    \"\"\"\n",
    "    Perform a single training step for the generator.\n",
    "    \n",
    "    Args:\n",
    "        generator (torch.nn.Module): The generator model.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating generator parameters.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The fake data generated during this training step.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    # Sample random latent vectors from a normal distribution\n",
    "    z = torch.randn(batch_size, latent_dim)\n",
    "    # Generate fake data from the latent vectors\n",
    "    fake_data = generator(z)\n",
    "    # Define a simple loss as the negative mean of the generator output\n",
    "    loss = -torch.mean(fake_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Detach generated data from the computation graph and convert to NumPy array\n",
    "    return fake_data.detach().numpy()\n",
    "\n",
    "def update(frame, x, real_pdf, fake_line, disc_line):\n",
    "    \"\"\"\n",
    "    Update function for the animation.\n",
    "    \n",
    "    Simulates the evolution of the generator's output and a discriminator's response.\n",
    "    \n",
    "    Args:\n",
    "        frame (int): Current frame index in the animation.\n",
    "        x (np.ndarray): Array of x-values for plotting.\n",
    "        real_pdf (np.ndarray): The probability density of the real distribution.\n",
    "        fake_line (matplotlib.lines.Line2D): Line object for the generator's output.\n",
    "        disc_line (matplotlib.lines.Line2D): Line object for the discriminator's output.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Updated fake_line and disc_line for the animation.\n",
    "    \"\"\"\n",
    "    # Simulate improvement in the generator's output over time:\n",
    "    gen_mean = 2 + frame * 0.1\n",
    "    gen_std = max(2 - frame * 0.05, 0.5)  # Prevent standard deviation from dropping too low\n",
    "    # Compute the generator's PDF with current parameters\n",
    "    gen_pdf = norm.pdf(x, gen_mean, gen_std)\n",
    "    \n",
    "    # Simulate a discriminator response using a logistic function on the difference\n",
    "    disc_prob = 1 / (1 + np.exp(-(gen_pdf - real_pdf) * 10))\n",
    "    \n",
    "    # Update the plot data for the generator and discriminator lines\n",
    "    fake_line.set_data(x, gen_pdf)\n",
    "    disc_line.set_data(x, disc_prob)\n",
    "    \n",
    "    return fake_line, disc_line\n",
    "\n",
    "# ---------------------------\n",
    "# Direct function calls below\n",
    "# ---------------------------\n",
    "\n",
    "# Build generator models using the functional approach\n",
    "generator1 = build_generator()\n",
    "generator2 = build_generator()\n",
    "\n",
    "# Initialize optimizers for each generator\n",
    "optimizer1 = optim.Adam(generator1.parameters(), lr=lr)\n",
    "optimizer2 = optim.Adam(generator2.parameters(), lr=lr)\n",
    "\n",
    "# Save ground truth data by sampling from the true distribution\n",
    "num_ground_truth_samples = 1000\n",
    "ground_truth_data = true_distribution(num_ground_truth_samples)\n",
    "\n",
    "# Initialize lists to store generated data from each generator\n",
    "generated_data_gen1 = []\n",
    "generated_data_gen2 = []\n",
    "\n",
    "# Training loop: generate fake data and save it in array objects\n",
    "for epoch in range(epochs):\n",
    "    fake_data1 = train_generator(generator1, optimizer1)\n",
    "    fake_data2 = train_generator(generator2, optimizer2)\n",
    "    generated_data_gen1.append(fake_data1)\n",
    "    generated_data_gen2.append(fake_data2)\n",
    "\n",
    "# ---------------------------\n",
    "# Setup for visualization/animation\n",
    "# ---------------------------\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 0.8)\n",
    "ax.set_title(\"Training Discriminator Simulation\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "\n",
    "# Create x values and compute the real probability density function (PDF)\n",
    "x = np.linspace(0, 10, 1000)\n",
    "real_pdf = norm.pdf(x, 5, 1)\n",
    "# Plot the real data distribution (dashed black line)\n",
    "ax.plot(x, real_pdf, 'k--', label='Real Data (pdata)')\n",
    "\n",
    "# Initialize empty plot lines for generator output and discriminator response\n",
    "fake_data_line, = ax.plot([], [], 'g-', label='Generator (pg)')\n",
    "discriminator_line, = ax.plot([], [], 'b--', label='Discriminator (D)')\n",
    "ax.legend()\n",
    "\n",
    "# Create animation; using a lambda to pass extra parameters to the update function\n",
    "ani = animation.FuncAnimation(\n",
    "    fig,\n",
    "    lambda frame: update(frame, x, real_pdf, fake_data_line, discriminator_line),\n",
    "    frames=30,\n",
    "    interval=200,\n",
    "    blit=True\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Save the animation in MP4 and GIF formats\n",
    "# ---------------------------\n",
    "# Create an FFMpegWriter object for MP4 saving\n",
    "mp4_writer = FFMpegWriter(\n",
    "    fps=10,\n",
    "    metadata=dict(artist='Your Name'),\n",
    "    extra_args=['-vcodec', 'libx264']\n",
    ")\n",
    "ani.save('gan_training_progress.mp4', writer=mp4_writer)\n",
    "\n",
    "# Save as GIF using the pillow writer\n",
    "ani.save('gan_training_progress.gif', writer='pillow', fps=10)\n",
    "#\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b933332a",
   "metadata": {},
   "source": [
    "# Explanation of the Code and Functional Analysis\n",
    "\n",
    "The code implements a training process for two independent generators, each attempting to approximate a given probability distribution through iterative optimization. The generators take latent noise vectors as input and transform them into data points resembling a target normal distribution. The loss function measures the discrepancy between the generator's output statistics (mean and standard deviation) and those of the real distribution, allowing the model to progressively refine its outputs. Additionally, the code includes an animated visualization of the training process, showing how the generators' probability density functions (PDFs) evolve over time. The real data distribution is defined as:\n",
    "\n",
    "$$\n",
    "X \\sim \\mathcal{N}(\\mu=5, \\sigma=1)\n",
    "$$\n",
    "\n",
    "where samples are drawn from a normal distribution with mean $\\mu = 5$ and standard deviation $\\sigma = 1$. The function `true_distribution(n)` generates $n$ such samples. Each generator model is built using a feedforward neural network with fully connected layers and ReLU activations. The network maps a latent vector $z$, sampled from a standard normal distribution:\n",
    "\n",
    "$$\n",
    "z \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "to a single-dimensional output, aiming to replicate the target distribution. The transformation follows:\n",
    "\n",
    "$$\n",
    "h_1 = \\text{ReLU}(W_1 z + b_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\text{ReLU}(W_2 h_1 + b_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = W_3 h_2 + b_3\n",
    "$$\n",
    "\n",
    "where $W_i$ and $b_i$ are the learnable weight matrices and bias vectors. The function `build_generator()` returns this neural network encapsulated within a `Sequential` model. The training process, defined in `train_generator(generator, optimizer)`, is structured as follows:\n",
    "\n",
    "1. A batch of latent vectors is sampled:\n",
    "\n",
    "   $$\n",
    "   Z \\sim \\mathcal{N}(0, I)\n",
    "   $$\n",
    "\n",
    "2. The generator transforms these latent vectors into synthetic data points:\n",
    "\n",
    "   $$\n",
    "   \\hat{X} = G(Z)\n",
    "   $$\n",
    "\n",
    "3. Instead of the traditional adversarial loss, the loss function is designed to minimize the difference between the generator’s output statistics and those of the target distribution:\n",
    "\n",
    "   $$\n",
    "   \\mathcal{L}_G = (\\mathbb{E}[\\hat{X}] - 5)^2 + (\\text{std}(\\hat{X}) - 1)^2\n",
    "   $$\n",
    "\n",
    "   where $\\mathbb{E}[\\hat{X}]$ and $\\text{std}(\\hat{X})$ are the mean and standard deviation of the generated samples. This loss ensures that the generator’s output converges toward the desired mean and standard deviation.\n",
    "\n",
    "Two independent generators (`generator1` and `generator2`) are initialized along with their respective Adam optimizers. The training loop executes for `epochs = 200`, iteratively updating each generator’s parameters based on the computed loss. The mean and standard deviation of the generated samples are stored at each epoch to analyze convergence.\n",
    "\n",
    "To visualize the training process, probability density functions (PDFs) of the generated data are computed over a fixed range of values. The evolution of these PDFs is animated using the `FuncAnimation` module, with updates reflecting changes in the mean and standard deviation of the generator outputs. The animation also displays the real data distribution as a reference, demonstrating how the generators gradually approximate it.\n",
    "\n",
    "The function `create_animation(x, real_pdf, gen_pdf1, gen_pdf2, losses1, losses2, epochs, interval=100)` constructs an animated visualization where each frame represents an epoch. The generator PDFs are plotted alongside the real data distribution, and a text annotation provides real-time loss updates. The animation is saved in both MP4 and GIF formats using `FFMpegWriter` and `pillow`. Overall, the code presents a novel training approach where the generator is optimized using distributional statistics rather than adversarial feedback. This method provides an alternative to Generative Adversarial Networks (GANs) by enforcing statistical convergence, allowing for a more stable training process without requiring a discriminator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b2b38a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T22:22:15.744034Z",
     "start_time": "2025-02-22T22:21:48.601036Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from scipy.stats import norm\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "latent_dim = 10       # Dimensionality of the latent (noise) vector\n",
    "hidden_dim = 128      # Number of neurons in the hidden layers\n",
    "data_dim = 1          # Output dimensionality (univariate distribution)\n",
    "batch_size = 128      # Number of samples per training batch\n",
    "epochs = 200          # Number of training epochs\n",
    "lr = 0.0002           # Learning rate for the optimizers\n",
    "\n",
    "# ---------------------------\n",
    "# Data Function\n",
    "# ---------------------------\n",
    "def true_distribution(n):\n",
    "    \"\"\"\n",
    "    Generate n samples from the real distribution (N(5,1)).\n",
    "    \n",
    "    Args:\n",
    "        n (int): Number of samples.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Samples of shape (n, 1).\n",
    "    \"\"\"\n",
    "    mean = 5\n",
    "    std = 1\n",
    "    return np.random.normal(mean, std, n).reshape(-1, 1)\n",
    "\n",
    "# ---------------------------\n",
    "# Generator Model Function (using nn.Sequential)\n",
    "# ---------------------------\n",
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Build the generator model.\n",
    "    \n",
    "    Returns:\n",
    "        torch.nn.Sequential: A simple feed-forward network.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(latent_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, data_dim)\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# Training Function\n",
    "# ---------------------------\n",
    "def train_generator(generator, optimizer):\n",
    "    \"\"\"\n",
    "    Train the generator for one step. Instead of the old loss (negative mean),\n",
    "    this convergence loss encourages the generator's batch statistics (mean and std)\n",
    "    to match the ground truth (mean=5, std=1).\n",
    "    \n",
    "    Args:\n",
    "        generator (torch.nn.Module): The generator model.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (loss_value, fake_data, sample_mean, sample_std)\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    z = torch.randn(batch_size, latent_dim)\n",
    "    fake_data = generator(z)\n",
    "    sample_mean = torch.mean(fake_data)\n",
    "    sample_std = torch.std(fake_data)\n",
    "    # Convergence loss: MSE for mean and std\n",
    "    loss = (sample_mean - 5)**2 + (sample_std - 1)**2\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), fake_data.detach().numpy(), sample_mean.item(), sample_std.item()\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop and Data Storage\n",
    "# ---------------------------\n",
    "# Build two generator models and corresponding optimizers.\n",
    "generator1 = build_generator()\n",
    "generator2 = build_generator()\n",
    "optimizer1 = optim.Adam(generator1.parameters(), lr=lr)\n",
    "optimizer2 = optim.Adam(generator2.parameters(), lr=lr)\n",
    "\n",
    "# Lists for storing loss and generator statistics for animation.\n",
    "losses1 = []\n",
    "losses2 = []\n",
    "stats1 = []  # Each element is a tuple: (mean, std) for generator1\n",
    "stats2 = []  # For generator2\n",
    "\n",
    "# Train both generators and print progress.\n",
    "for epoch in range(epochs):\n",
    "    loss1, fake1, mean1, std1 = train_generator(generator1, optimizer1)\n",
    "    loss2, fake2, mean2, std2 = train_generator(generator2, optimizer2)\n",
    "    losses1.append(loss1)\n",
    "    losses2.append(loss2)\n",
    "    stats1.append((mean1, std1))\n",
    "    stats2.append((mean2, std2))\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}/{epochs} | Loss1: {loss1:.4f} | Loss2: {loss2:.4f} | \"\n",
    "              f\"Mean1: {mean1:.4f}, Std1: {std1:.4f} | Mean2: {mean2:.4f}, Std2: {std2:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Compute PDFs for Animation\n",
    "# ---------------------------\n",
    "# Define x-axis for plotting the PDFs.\n",
    "x = np.linspace(0, 10, 1000)\n",
    "# The real PDF (ground truth) is constant.\n",
    "real_pdf = norm.pdf(x, 5, 1)\n",
    "# Compute the generator PDFs at each epoch from stored statistics.\n",
    "gen_pdf1 = [norm.pdf(x, m, s) for m, s in stats1]\n",
    "gen_pdf2 = [norm.pdf(x, m, s) for m, s in stats2]\n",
    "gen_pdf1 = np.array(gen_pdf1)\n",
    "gen_pdf2 = np.array(gen_pdf2)\n",
    "\n",
    "# ---------------------------\n",
    "# Animation Functions\n",
    "# ---------------------------\n",
    "def create_animation(x, real_pdf, gen_pdf1, gen_pdf2, losses1, losses2, epochs, interval=100):\n",
    "    \"\"\"\n",
    "    Create an animation showing the evolution of the generator PDFs and loss.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): x-axis values.\n",
    "        real_pdf (np.ndarray): Real PDF values.\n",
    "        gen_pdf1 (np.ndarray): Generator 1 PDF for each epoch.\n",
    "        gen_pdf2 (np.ndarray): Generator 2 PDF for each epoch.\n",
    "        losses1 (list): Loss values for generator1.\n",
    "        losses2 (list): Loss values for generator2.\n",
    "        epochs (int): Total number of epochs (frames).\n",
    "        interval (int): Delay between frames (ms).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (ani, fig) the animation object and the figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(np.min(x), np.max(x))\n",
    "    ax.set_ylim(0, 0.8)\n",
    "    ax.set_title(\"Training Progress: Generator Convergence\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    \n",
    "    # Plot the fixed real data PDF.\n",
    "    ax.plot(x, real_pdf, 'k--', label=\"Real Data (pdata)\")\n",
    "    line1, = ax.plot([], [], 'g-', label=\"Generator 1 (pg)\")\n",
    "    line2, = ax.plot([], [], 'b-', label=\"Generator 2 (pg)\")\n",
    "    \n",
    "    # Text annotation for loss information.\n",
    "    loss_text = ax.text(0.05, 0.75, \"\", transform=ax.transAxes)\n",
    "    ax.legend()\n",
    "    \n",
    "    def update(frame):\n",
    "        line1.set_data(x, gen_pdf1[frame])\n",
    "        line2.set_data(x, gen_pdf2[frame])\n",
    "        loss_text.set_text(f\"Epoch: {frame+1}\\nLoss1: {losses1[frame]:.4f}\\nLoss2: {losses2[frame]:.4f}\")\n",
    "        return line1, line2, loss_text\n",
    "    \n",
    "    ani = animation.FuncAnimation(fig, update, frames=epochs, interval=interval, blit=True)\n",
    "    return ani, fig\n",
    "\n",
    "# Create the animation.\n",
    "ani, fig = create_animation(x, real_pdf, gen_pdf1, gen_pdf2, losses1, losses2, epochs, interval=100)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Save the Animation\n",
    "# ---------------------------\n",
    "def save_animation(ani, mp4_filename, gif_filename, fps=10):\n",
    "    \"\"\"\n",
    "    Save the animation in both MP4 and GIF formats.\n",
    "    \n",
    "    Args:\n",
    "        ani (FuncAnimation): The animation object.\n",
    "        mp4_filename (str): Output file name for MP4.\n",
    "        gif_filename (str): Output file name for GIF.\n",
    "        fps (int): Frames per second.\n",
    "    \"\"\"\n",
    "    writer = FFMpegWriter(fps=fps, metadata=dict(artist='Your Name'),\n",
    "                           extra_args=['-vcodec', 'libx264'])\n",
    "    ani.save(mp4_filename, writer=writer)\n",
    "    ani.save(gif_filename, writer='pillow', fps=fps)\n",
    "\n",
    "# Save the animation.\n",
    "save_animation(ani, \"gan_training_progress.mp4\", \"gan_training_progress.gif\", fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f5df1",
   "metadata": {},
   "source": [
    "# Explanation of the Code and Functional Analysis\n",
    "\n",
    "The provided code implements a Generative Adversarial Network (GAN) to approximate a univariate normal distribution. The GAN consists of two neural networks: a generator that transforms latent noise into synthetic data samples and a discriminator that classifies inputs as real or fake. The training process alternates between updating the discriminator to distinguish real from fake data and optimizing the generator to produce samples that closely resemble the true distribution. Additionally, the code includes an animated visualization to illustrate the convergence of the generator's probability density function (PDF) over training epochs. The real data is sampled from a normal distribution:\n",
    "\n",
    "$$\n",
    "X \\sim \\mathcal{N}(\\mu=5, \\sigma=1)\n",
    "$$\n",
    "\n",
    "where the function `true_distribution(n)` generates $n$ samples. The generator network is designed as a fully connected feed-forward model that maps a latent vector $z$ sampled from:\n",
    "\n",
    "$$\n",
    "z \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "to an output approximating the real data distribution. Its transformation follows:\n",
    "\n",
    "$$\n",
    "h_1 = \\text{ReLU}(W_1 z + b_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\text{ReLU}(W_2 h_1 + b_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = W_3 h_2 + b_3\n",
    "$$\n",
    "\n",
    "where $W_i$ and $b_i$ are the network's weight matrices and bias vectors. The discriminator is a similar feed-forward network that receives either real or fake samples and outputs a probability score using a sigmoid activation function. The discriminator training function `train_discriminator()` updates its weights by minimizing the binary cross-entropy (BCE) loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_D = - \\mathbb{E}[\\log D(x_{\\text{real}})] - \\mathbb{E}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "where $D(x_{\\text{real}})$ is the discriminator's probability estimate for real data, and $D(G(z))$ is its estimate for fake data. The generator training function `train_generator()` minimizes a combined loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_G = \\mathcal{L}_{\\text{adv}} + \\mathcal{L}_{\\text{conv}}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}_{\\text{adv}}$ is the adversarial loss, encouraging the generator to produce realistic data:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{adv}} = - \\mathbb{E}[\\log D(G(z))]\n",
    "$$\n",
    "\n",
    "and $\\mathcal{L}_{\\text{conv}}$ is a statistical loss ensuring the generator's output mean and standard deviation converge to the target distribution:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{conv}} = (\\mathbb{E}[G(z)] - 5)^2 + (\\text{std}(G(z)) - 1)^2\n",
    "$$\n",
    "\n",
    "The training loop runs for `epochs = 200`, iteratively updating the discriminator and generator. At each epoch, statistics (mean and standard deviation) of the generated samples are recorded, and the generator's output PDF is computed. The function `create_animation()` visualizes the evolution of the generator's PDF, the discriminator's response function, and the loss values over time. \n",
    "\n",
    "By introducing a convergence loss alongside the adversarial loss, this GAN stabilizes training and ensures that the generator learns not only to fool the discriminator but also to match the real data distribution statistically. The animation illustrates this progression, showing how the generator’s output gradually aligns with the target normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21af116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T22:23:03.219037Z",
     "start_time": "2025-02-22T22:22:36.070036Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from scipy.stats import norm\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "latent_dim = 10       # Dimensionality of latent vector\n",
    "hidden_dim = 128      # Hidden layer size\n",
    "data_dim = 1          # Output dimension (univariate)\n",
    "batch_size = 128      # Batch size for training\n",
    "epochs = 200          # Number of training epochs\n",
    "lr = 0.0002           # Learning rate\n",
    "\n",
    "# ---------------------------\n",
    "# Data Function\n",
    "# ---------------------------\n",
    "def true_distribution(n):\n",
    "    \"\"\"\n",
    "    Generate n samples from N(5, 1).\n",
    "    \n",
    "    Args:\n",
    "        n (int): Number of samples.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Samples of shape (n, 1).\n",
    "    \"\"\"\n",
    "    mean, std = 5, 1\n",
    "    return np.random.normal(mean, std, n).reshape(-1, 1)\n",
    "\n",
    "# ---------------------------\n",
    "# Model Building Functions (using nn.Sequential)\n",
    "# ---------------------------\n",
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Build a generator model.\n",
    "    \n",
    "    Returns:\n",
    "        torch.nn.Sequential: A feed-forward network mapping latent vectors to data.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(latent_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, data_dim)\n",
    "    )\n",
    "\n",
    "def build_discriminator():\n",
    "    \"\"\"\n",
    "    Build a discriminator model.\n",
    "    \n",
    "    Returns:\n",
    "        torch.nn.Sequential: A feed-forward network mapping data to a probability.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(data_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# Training Step Functions\n",
    "# ---------------------------\n",
    "def train_discriminator(discriminator, generator, D_optimizer, criterion, real_data):\n",
    "    \"\"\"\n",
    "    Train the discriminator for one step.\n",
    "    \n",
    "    Args:\n",
    "        discriminator (torch.nn.Module): The discriminator model.\n",
    "        generator (torch.nn.Module): The generator model.\n",
    "        D_optimizer (torch.optim.Optimizer): Optimizer for the discriminator.\n",
    "        criterion: Loss function (BCE).\n",
    "        real_data (torch.Tensor): Batch of real data.\n",
    "    \n",
    "    Returns:\n",
    "        float: Discriminator loss.\n",
    "    \"\"\"\n",
    "    D_optimizer.zero_grad()\n",
    "    # Real samples and labels\n",
    "    real_labels = torch.ones(real_data.size(0), 1)\n",
    "    real_preds = discriminator(real_data)\n",
    "    loss_real = criterion(real_preds, real_labels)\n",
    "    \n",
    "    # Fake samples\n",
    "    z = torch.randn(real_data.size(0), latent_dim)\n",
    "    fake_data = generator(z)\n",
    "    fake_labels = torch.zeros(real_data.size(0), 1)\n",
    "    fake_preds = discriminator(fake_data.detach())\n",
    "    loss_fake = criterion(fake_preds, fake_labels)\n",
    "    \n",
    "    D_loss = loss_real + loss_fake\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "    return D_loss.item()\n",
    "\n",
    "def train_generator(generator, discriminator, G_optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Train the generator for one step. The generator loss is the sum of:\n",
    "      - an adversarial loss (to fool the discriminator)\n",
    "      - a convergence loss (to match the ground truth: mean=5, std=1)\n",
    "    \n",
    "    Args:\n",
    "        generator (torch.nn.Module): The generator model.\n",
    "        discriminator (torch.nn.Module): The discriminator model.\n",
    "        G_optimizer (torch.optim.Optimizer): Optimizer for the generator.\n",
    "        criterion: Loss function (BCE) for adversarial loss.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (generator loss, fake data, sample_mean, sample_std)\n",
    "    \"\"\"\n",
    "    G_optimizer.zero_grad()\n",
    "    z = torch.randn(batch_size, latent_dim)\n",
    "    fake_data = generator(z)\n",
    "    fake_preds = discriminator(fake_data)\n",
    "    # Adversarial loss: try to fool the discriminator\n",
    "    real_labels = torch.ones(batch_size, 1)\n",
    "    adv_loss = criterion(fake_preds, real_labels)\n",
    "    \n",
    "    # Convergence loss: force sample statistics to match ground truth (mean=5, std=1)\n",
    "    sample_mean = torch.mean(fake_data)\n",
    "    sample_std = torch.std(fake_data)\n",
    "    conv_loss = (sample_mean - 5)**2 + (sample_std - 1)**2\n",
    "    \n",
    "    G_loss = adv_loss + conv_loss\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "    \n",
    "    return G_loss.item(), fake_data.detach().numpy(), sample_mean.item(), sample_std.item()\n",
    "\n",
    "# ---------------------------\n",
    "# Simulation of Training and Data Storage\n",
    "# ---------------------------\n",
    "# Build models and optimizers\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "G_optimizer = optim.Adam(generator.parameters(), lr=lr)\n",
    "D_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# For plotting, define an x-axis and compute the ground truth PDF (constant)\n",
    "x = np.linspace(0, 10, 1000)\n",
    "real_pdf = norm.pdf(x, 5, 1)\n",
    "\n",
    "# Lists to store data for animation\n",
    "ground_truth_data = []         # Will store real_pdf (constant) per epoch\n",
    "generator_data = []            # Generator PDF computed from sample statistics\n",
    "discriminator_data = []        # Discriminator output on x (grid)\n",
    "losses_G = []                  # Generator losses\n",
    "losses_D = []                  # Discriminator losses\n",
    "gen_stats = []                 # (mean, std) from generator\n",
    "\n",
    "# Pre-store ground truth (same for all epochs)\n",
    "for _ in range(epochs):\n",
    "    ground_truth_data.append(real_pdf)\n",
    "ground_truth_data = np.array(ground_truth_data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Generate real data batch\n",
    "    real_batch = torch.tensor(true_distribution(batch_size), dtype=torch.float32)\n",
    "    # Train discriminator\n",
    "    D_loss = train_discriminator(discriminator, generator, D_optimizer, criterion, real_batch)\n",
    "    # Train generator\n",
    "    G_loss, fake_data, sample_mean, sample_std = train_generator(generator, discriminator, G_optimizer, criterion)\n",
    "    \n",
    "    losses_D.append(D_loss)\n",
    "    losses_G.append(G_loss)\n",
    "    gen_stats.append((sample_mean, sample_std))\n",
    "    \n",
    "    # Compute generator PDF using the sample mean and std from this epoch\n",
    "    gen_pdf = norm.pdf(x, sample_mean, sample_std)\n",
    "    generator_data.append(gen_pdf)\n",
    "    \n",
    "    # Evaluate discriminator on the grid x\n",
    "    x_tensor = torch.tensor(x.reshape(-1, 1), dtype=torch.float32)\n",
    "    disc_out = discriminator(x_tensor).detach().numpy().flatten()\n",
    "    discriminator_data.append(disc_out)\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}/{epochs} | D_loss: {D_loss:.4f} | G_loss: {G_loss:.4f} | \"\n",
    "              f\"Gen_Mean: {sample_mean:.4f}, Gen_Std: {sample_std:.4f}\")\n",
    "\n",
    "# Convert lists to arrays for animation\n",
    "generator_data = np.array(generator_data)\n",
    "discriminator_data = np.array(discriminator_data)\n",
    "\n",
    "# ---------------------------\n",
    "# Animation Functions\n",
    "# ---------------------------\n",
    "def create_animation(x, ground_truth_data, generator_data, discriminator_data, losses_G, losses_D, epochs, interval=100):\n",
    "    \"\"\"\n",
    "    Create an animation showing the evolution of the generator and discriminator outputs\n",
    "    along with loss values.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): x-axis values.\n",
    "        ground_truth_data (np.ndarray): Ground truth PDF (epochs x len(x)).\n",
    "        generator_data (np.ndarray): Generator PDFs (epochs x len(x)).\n",
    "        discriminator_data (np.ndarray): Discriminator outputs on x (epochs x len(x)).\n",
    "        losses_G (list): Generator losses per epoch.\n",
    "        losses_D (list): Discriminator losses per epoch.\n",
    "        epochs (int): Number of epochs (frames).\n",
    "        interval (int): Delay between frames in ms.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ani, fig) the animation and the figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(np.min(x), np.max(x))\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(\"GAN Training Progress\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    \n",
    "    # Plot constant real PDF\n",
    "    ax.plot(x, ground_truth_data[0], 'k--', label=\"Real Data (pdata)\")\n",
    "    gen_line, = ax.plot([], [], 'g-', label=\"Generator (pg)\")\n",
    "    disc_line, = ax.plot([], [], 'b--', label=\"Discriminator (D)\")\n",
    "    \n",
    "    # Annotation text for epoch and losses\n",
    "    info_text = ax.text(0.05, 0.80, \"\", transform=ax.transAxes)\n",
    "    ax.legend()\n",
    "    \n",
    "    def update(frame):\n",
    "        gen_line.set_data(x, generator_data[frame])\n",
    "        disc_line.set_data(x, discriminator_data[frame])\n",
    "        info_text.set_text(\n",
    "            f\"Epoch: {frame+1}\\nG_loss: {losses_G[frame]:.4f}\\nD_loss: {losses_D[frame]:.4f}\"\n",
    "        )\n",
    "        return gen_line, disc_line, info_text\n",
    "    \n",
    "    ani = animation.FuncAnimation(fig, update, frames=epochs, interval=interval, blit=True)\n",
    "    return ani, fig\n",
    "\n",
    "# Create and display the animation\n",
    "ani, fig = create_animation(x, ground_truth_data, generator_data, discriminator_data, losses_G, losses_D, epochs, interval=100)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Save the Animation\n",
    "# ---------------------------\n",
    "def save_animation(ani, mp4_filename, gif_filename, fps=10):\n",
    "    \"\"\"\n",
    "    Save the animation as MP4 and GIF.\n",
    "    \n",
    "    Args:\n",
    "        ani (FuncAnimation): The animation object.\n",
    "        mp4_filename (str): Filename for MP4.\n",
    "        gif_filename (str): Filename for GIF.\n",
    "        fps (int): Frames per second.\n",
    "    \"\"\"\n",
    "    writer = FFMpegWriter(fps=fps, metadata=dict(artist='Your Name'),\n",
    "                           extra_args=['-vcodec', 'libx264'])\n",
    "    ani.save(mp4_filename, writer=writer)\n",
    "    ani.save(gif_filename, writer='pillow', fps=fps)\n",
    "\n",
    "# Save the animation in both formats\n",
    "save_animation(ani, \"gan_training_progress.mp4\", \"gan_training_progress.gif\", fps=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d1d96",
   "metadata": {},
   "source": [
    "# Explanation of the Code and Functional Analysis\n",
    "\n",
    "The provided code implements a generative training process where two independent generator models attempt to learn and approximate a two-dimensional normal distribution through iterative updates. The system uses artificial neural networks with fully connected layers, ReLU activation functions, and an Adam optimizer for parameter updates. The training progress is visually represented via an animated scatter plot that illustrates the evolution of the generated data distributions over time. The true data distribution is defined as a bivariate normal distribution:\n",
    "\n",
    "$$\n",
    "X \\sim \\mathcal{N}(\\mu, \\Sigma)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix} 2 \\\\ -2 \\end{bmatrix}, \\quad\n",
    "\\Sigma = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This distribution is generated using the function `true_distribution(n)`, which samples $n$ points from the specified multivariate normal distribution. These points serve as the real data samples that the generators aim to emulate. Each generator model is constructed using the function `build_generator()`, which returns a neural network structured as follows:\n",
    "\n",
    "$$\n",
    "h_1 = \\text{ReLU}(W_1 z + b_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\text{ReLU}(W_2 h_1 + b_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x} = W_3 h_2 + b_3\n",
    "$$\n",
    "\n",
    "where $z \\sim \\mathcal{N}(0, I)$ is a latent noise vector sampled from a standard normal distribution. The model maps this latent space to a two-dimensional output that approximates the real data distribution. The function `train_generator(generator, optimizer)` executes a single training step for the generator by first sampling a batch of latent vectors, passing them through the generator model, and computing a loss function defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_G = -\\mathbb{E}[G(z)]\n",
    "$$\n",
    "\n",
    "This proxy loss encourages the generator’s output to shift towards larger values over time, pushing its generated distribution to better approximate the real data distribution. The loss is minimized through backpropagation, and the optimizer updates the model parameters accordingly. During training, both generators (`generator1` and `generator2`) are independently optimized for `epochs = 300`. At each epoch, the generated samples are recorded and stored in `generated_data_gen1` and `generated_data_gen2` for later visualization.\n",
    "\n",
    "To illustrate the training progression, the function `update(frame, fake_scatter_G1, fake_scatter_G2)` updates the positions of two scatter plots corresponding to the data generated by each generator. The animation function `FuncAnimation` calls this update function iteratively, creating a dynamic visualization of how the synthetic data distributions evolve and gradually align with the real distribution. The visualization consists of a scatter plot overlayed on a contour plot representing the probability density function (PDF) of the true data distribution:\n",
    "\n",
    "$$\n",
    "p(x, y) = \\frac{1}{2\\pi |\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\n",
    "$$\n",
    "\n",
    "This contour plot provides a reference against which the generator outputs can be compared throughout training.\n",
    "\n",
    "Finally, the animation is saved in both MP4 and GIF formats using `FFMpegWriter` and the `pillow` library, ensuring compatibility with various media formats. The resulting animation demonstrates the progressive improvement of the generators as they refine their output distributions toward the target distribution.\n",
    "\n",
    "Overall, this implementation serves as a simplified generative modeling framework where neural networks learn to approximate a two-dimensional normal distribution through iterative training. The visualization component provides an intuitive understanding of the generator’s learning process, revealing how generated samples converge towards the true distribution over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d8416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-22T17:45:29.516340Z",
     "start_time": "2025-02-22T17:45:08.791343Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib as mpl\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "from scipy.stats import multivariate_normal\n",
    "%matplotlib notebook\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters (tuned for better results)\n",
    "# ---------------------------\n",
    "latent_dim = 10         # Dimensionality of the latent (noise) vector\n",
    "hidden_dim = 256        # Increased number of neurons in hidden layers for better capacity\n",
    "data_dim = 2            # 2D data points\n",
    "batch_size = 256        # Increased batch size for more stable gradient estimates\n",
    "epochs = 300            # Increased epochs to allow more training iterations\n",
    "lr = 0.001              # Increased learning rate for faster convergence\n",
    "\n",
    "def true_distribution(n):\n",
    "    \"\"\"\n",
    "    Generate samples from the true (real) 2D distribution.\n",
    "    Args:\n",
    "        n (int): Number of samples to generate.\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (n, 2) drawn from a multivariate normal\n",
    "                    distribution with mean [2, -2] and identity covariance.\n",
    "    \"\"\"\n",
    "    mean = [2, -2]\n",
    "    cov = [[1, 0], [0, 1]]\n",
    "    return np.random.multivariate_normal(mean, cov, n)\n",
    "\n",
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Build the generator model using a functional (sequential) approach.\n",
    "    Returns:\n",
    "        torch.nn.Sequential: Generator model mapping latent vectors to 2D data points.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(latent_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, data_dim),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_generator(generator, optimizer):\n",
    "    \"\"\"\n",
    "    Perform a single training step for the generator.\n",
    "    The generator is trained to maximize the mean of its output, a proxy loss\n",
    "    that, along with the tuned hyperparameters, helps it better approximate the true distribution.\n",
    "    Args:\n",
    "        generator (torch.nn.Module): The generator model.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for the generator.\n",
    "    Returns:\n",
    "        np.ndarray: Generated fake data from the current training step.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    # Sample random latent vectors\n",
    "    z = torch.randn(batch_size, latent_dim)\n",
    "    fake_data = generator(z)\n",
    "    # Loss defined as negative mean of generated data (a proxy for improvement)\n",
    "    loss = -torch.mean(fake_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return fake_data.detach().numpy()\n",
    "\n",
    "def update(frame, fake_scatter_G1, fake_scatter_G2):\n",
    "    \"\"\"\n",
    "    Update function for the animation. This function updates the scatter plot positions \n",
    "    for both generator outputs based on the data saved during training.\n",
    "    Args:\n",
    "        frame (int): Current frame index.\n",
    "        fake_scatter_G1 (matplotlib.collections.PathCollection): Scatter plot for generator 1.\n",
    "        fake_scatter_G2 (matplotlib.collections.PathCollection): Scatter plot for generator 2.\n",
    "    Returns:\n",
    "        tuple: Updated scatter plot objects for the animation.\n",
    "    \"\"\"\n",
    "    # Update the offsets of the scatter plots with the generated data for the current frame\n",
    "    fake_scatter_G1.set_offsets(generated_data_gen1[frame])\n",
    "    fake_scatter_G2.set_offsets(generated_data_gen2[frame])\n",
    "    return fake_scatter_G1, fake_scatter_G2\n",
    "\n",
    "# ---------------------------\n",
    "# Set the ffmpeg executable path in rcParams\n",
    "# ---------------------------\n",
    "mpl.rcParams['animation.ffmpeg_path'] = r'C:\\Users\\Osvaldo\\anaconda3\\envs\\Oz\\Library\\bin\\ffmpeg.exe'\n",
    "\n",
    "# ---------------------------\n",
    "# Direct function calls (no main function)\n",
    "# ---------------------------\n",
    "\n",
    "# Build generator models using a functional approach\n",
    "generator1 = build_generator()\n",
    "generator2 = build_generator()\n",
    "\n",
    "# Initialize optimizers for each generator\n",
    "optimizer1 = optim.Adam(generator1.parameters(), lr=lr)\n",
    "optimizer2 = optim.Adam(generator2.parameters(), lr=lr)\n",
    "\n",
    "# Save ground truth data by sampling from the true distribution\n",
    "num_ground_truth_samples = 1000\n",
    "ground_truth_data = true_distribution(num_ground_truth_samples)\n",
    "\n",
    "# Initialize lists to store generated data for each generator\n",
    "generated_data_gen1 = []\n",
    "generated_data_gen2 = []\n",
    "\n",
    "# Training loop: update generators and record their outputs for each epoch\n",
    "for epoch in range(epochs):\n",
    "    fake_data1 = train_generator(generator1, optimizer1)\n",
    "    fake_data2 = train_generator(generator2, optimizer2)\n",
    "    generated_data_gen1.append(fake_data1)\n",
    "    generated_data_gen2.append(fake_data2)\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization / Animation Setup\n",
    "# ---------------------------\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-4, 6)\n",
    "ax.set_ylim(-6, 2)\n",
    "ax.set_title(\"GAN Training Progress\")\n",
    "ax.set_xlabel(\"X-axis\")\n",
    "ax.set_ylabel(\"Y-axis\")\n",
    "\n",
    "# Generate grid for the ground truth probability density function (PDF)\n",
    "x, y = np.mgrid[-4:6:.1, -6:2:.1]\n",
    "pos = np.dstack((x, y))\n",
    "rv = multivariate_normal(mean=[2, -2], cov=[[1, 0], [0, 1]])\n",
    "# Plot the ground truth distribution as a contour plot\n",
    "ax.contourf(x, y, rv.pdf(pos), cmap='Reds', alpha=0.5)\n",
    "\n",
    "# Initialize scatter plots for the generators\n",
    "fake_scatter_G1 = ax.scatter([], [], color='blue', alpha=0.5, label='Generator 1')\n",
    "fake_scatter_G2 = ax.scatter([], [], color='green', alpha=0.5, label='Generator 2')\n",
    "ax.legend()\n",
    "\n",
    "# Create animation; the update function is called for each epoch/frame\n",
    "ani = animation.FuncAnimation(\n",
    "    fig,\n",
    "    lambda frame: update(frame, fake_scatter_G1, fake_scatter_G2),\n",
    "    frames=len(generated_data_gen1),\n",
    "    interval=100,\n",
    "    blit=True\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Save the animation in MP4 and GIF formats\n",
    "# ---------------------------\n",
    "# Create an FFMpegWriter object for MP4 saving\n",
    "mp4_writer = FFMpegWriter(\n",
    "    fps=10,\n",
    "    metadata=dict(artist='Your Name'),\n",
    "    extra_args=['-vcodec', 'libx264']\n",
    ")\n",
    "ani.save('gan_training_progress.mp4', writer=mp4_writer)\n",
    "\n",
    "# Save as GIF using the pillow writer\n",
    "ani.save('gan_training_progress.gif', writer='pillow', fps=10)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
